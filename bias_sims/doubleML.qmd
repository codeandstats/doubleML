---
title: "Manual DoubleML with XGBoost and Cross-Fitting"
format: 
  html:
    code-fold: true
editor: visual
---

::: {.callout-note title="Goal"}
Estimate the causal effect of a treatment \( D \) on an outcome \( Y \), controlling for high-dimensional confounders \( X \), using a manual implementation of Double Machine Learning with cross-fitting.
Also compare to naive OLS and non-cross-fitted DML.
:::

::: {.callout-tip title="Overview"}
We simulate data where \( Y = \theta D + f(X) + \varepsilon \), and estimate \( \theta \) using residual-on-residual regression after partialing out \( X \) using XGBoost.
:::


## Libraries

```{r}
library(xgboost)
#library(tidyverse)
library(ggplot2)
#library(dplyr)
library(tidyr)
library(caret)
library(DoubleML) # for make_plr_CCDDHNR2018
library(mvtnorm)
library(foreach)
library(doParallel)
``` 


## Simulation and Estimation Function with Cross-Fitting

```{r}
manual_doubleml_cf <- function(n = 500, p = 10, theta = 0.5, K = 2, seed = NULL, dgp = c("linear_uncorrelated", "linear_correlated", "nonlinear")) {
  if (!is.null(seed)) set.seed(seed)
  dgp <- match.arg(dgp)

  if (dgp == "nonlinear") {
    dat <- make_plr_CCDDHNR2018(n_obs = n)
    X <- as.matrix(dat$X)
    D <- dat$D
    Y <- dat$Y
  } else {
    if (dgp == "linear_uncorrelated") {
      X <- matrix(rnorm(n * p), nrow = n, ncol = p)
    } else if (dgp == "linear_correlated") {
      Sigma <- 0.5 ^ abs(outer(1:p, 1:p, "-"))  # AR(1) structure
      X <- rmvnorm(n, sigma = Sigma)
    }

    beta_shared <- rnorm(p)
    D <- X %*% beta_shared + rnorm(n)
    Y <- theta * D + X %*% beta_shared + rnorm(n)
  }

  # Create folds
  folds <- createFolds(1:n, k = K, list = TRUE, returnTrain = FALSE)
  D_tilde <- rep(NA, n)
  Y_tilde <- rep(NA, n)

  for (k in 1:K) {
    test_idx <- folds[[k]]
    train_idx <- setdiff(1:n, test_idx)

    # D ~ X
    dtrain_D <- xgb.DMatrix(data = X[train_idx, ], label = D[train_idx])
    model_D <- xgboost(data = dtrain_D, nrounds = 20, objective = "reg:squarederror", verbose = 0)
    D_hat <- predict(model_D, newdata = X[test_idx, ])

    # Y ~ X
    dtrain_Y <- xgb.DMatrix(data = X[train_idx, ], label = Y[train_idx])
    model_Y <- xgboost(data = dtrain_Y, nrounds = 20, objective = "reg:squarederror", verbose = 0)
    Y_hat <- predict(model_Y, newdata = X[test_idx, ])

    D_tilde[test_idx] <- D[test_idx] - D_hat
    Y_tilde[test_idx] <- Y[test_idx] - Y_hat
  }

  # Final stage OLS (cross-fitted)
  theta_hat_cf <- unname(coef(lm(Y_tilde ~ D_tilde))[2])

  # Naive OLS
  naive_theta <- unname(coef(lm(Y ~ D))[2])

  # No cross-fitting: train on full sample
  model_D_full <- xgboost(data = xgb.DMatrix(data = X, label = D), nrounds = 20, objective = "reg:squarederror", verbose = 0)
  D_hat_full <- predict(model_D_full, newdata = X)

  model_Y_full <- xgboost(data = xgb.DMatrix(data = X, label = Y), nrounds = 20, objective = "reg:squarederror", verbose = 0)
  Y_hat_full <- predict(model_Y_full, newdata = X)

  D_tilde_ncf <- D - D_hat_full
  Y_tilde_ncf <- Y - Y_hat_full

  theta_hat_ncf <- unname(coef(lm(Y_tilde_ncf ~ D_tilde_ncf))[2])

  return(c(theta_hat_cf = theta_hat_cf, theta_hat_ncf = theta_hat_ncf, naive_theta = naive_theta))
}
```


## Run Once

```{r}
manual_doubleml_cf(seed = 123, dgp = "nonlinear")
```


## Replicate to Show Distribution (Parallelized with foreach)

```{r}
B <- 500
n_cores <- parallel::detectCores() - 1
cl <- makeCluster(n_cores)
registerDoParallel(cl)
clusterExport(cl, varlist = c("manual_doubleml_cf")) 

results <- foreach(i = 1:B, .combine = rbind, .packages = c("xgboost", "caret", "DoubleML", "mvtnorm")) %dopar% {
  manual_doubleml_cf(seed = sample.int(1e6, 1), dgp = "linear_correlated")
}

stopCluster(cl)

results_df <- as_tibble(results)
colnames(results_df) <- c("theta_hat_cf", "theta_hat_ncf", "naive_theta")

results_df_long <- pivot_longer(results_df, cols = everything(), names_to = "method", values_to = "estimate")

# Plot

ggplot(results_df_long, aes(x = estimate, fill = method)) +
  geom_histogram(position = "identity", alpha = 0.6, bins = 30, color = "black") +
  geom_vline(xintercept = 0.5, color = "red", linetype = "dashed") +
  facet_wrap(~method, scales = "free") +
  labs(title = "Distribution of Estimates: DoubleML vs Naive OLS",
       x = "Estimated Theta", y = "Frequency")
```


## Bias and Variance Summary

```{r}
results_df %>%
  summarise(across(everything(), list(mean = mean, sd = sd)))
```


## Performance as a Function of Sample Size

```{r}
n_vals <- c(200, 500, 1000)

perf_by_n <- map_dfr(n_vals, function(n_val) {
  temp_res <- foreach(i = 1:200, .combine = rbind, .packages = c("xgboost", "caret", "DoubleML", "mvtnorm")) %dopar% {
    manual_doubleml_cf(n = n_val, seed = sample.int(1e6, 1), dgp = "linear_correlated")
  }
  tibble(n = n_val, method = rep(c("cf", "ncf", "naive"), each = 200), estimate = as.vector(t(temp_res)))
})

# Plot bias-variance tradeoff

ggplot(perf_by_n, aes(x = estimate, fill = method)) +
  geom_histogram(position = "identity", alpha = 0.5, bins = 30) +
  facet_grid(method ~ n, scales = "free") +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red") +
  labs(title = "Distribution of Estimates Across Sample Sizes", x = "Estimate", y = "Frequency")
```
